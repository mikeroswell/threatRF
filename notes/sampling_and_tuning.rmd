---
title: "testing_and_validation"
author: "Michael Roswell"
date: "2/2/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE
                      , dev = "ragg_png")
knitr::opts_knit$set(root.dir = '../') 
library(tidyverse)
library(caret)
library(doParallel)
library(pROC)
library(ROCR)
library(tictoc)

```
# Overview 
I have been overwhelmed by the number of steps at which decisions have
to get made when doing predictive modelling, and wanted to step back to review
options at each of those steps, and get a picture of both the literature answers
to which one/ combination to prefer and why, as well as behavior with a dataset
that closely ressembles the one I"m ultimatley trying to learn from.


# Data

I downloaded this dataset
https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset/download
to have an unbalanced, simulated dataset to work with. First, just going to look
at it really quickly.

```{r load and peek, results = FALSE}

# load data
dat<-read.csv("data/WA_Fn-UseC_-HR-Employee-Attrition.csv")
head(dat)
str(dat)
# this is the target
table(dat$Attrition)
set.seed(999)

# there are more cases here, but also the clss imbalance is more severe. 
# before continuing, let's just drop a ton of data from the "No" class. 

no_row<-which(dat$Attrition=="No")
yes_row<-which(dat$Attrition == "Yes")
nkeep<-sample(no_row, 200)
ykeep<-sample(yes_row, 100)
example_train_dat<-bind_rows(
  # the nos
  dat[ykeep,]
  # te yeses
  , dat[nkeep, ]
)

test_dat<-dat[-c(ykeep, nkeep),]

# reordere so it's not all yeses or nos

test_dat<-test_dat[sample(1:nrow(test_dat), replace =T),]

example_train_dat <- example_train_dat[sample(1:nrow(example_train_dat), replace =F),]

```


# Objectives 

Overall, I want to use Random Forest to create a predictive model for
classification. In setting this up, there are a few decisions to make, and
because there are so few data, I'm not sure I can just try things and test them
to evaluate which decision is best in a reasonable manner.

Three basic questions are:

1. How important is model tuning in this context? Based on this
[article](https://bmcmedinformdecismak.biomedcentral.com/track/pdf/10.1186/s12911-021-01688-3.pdf),
and [this](https://doi.org/10.1002/widm.1301) one, I'm wondering if with
datasets this small, will the gain in predictive power from tuning be too low/
hard to assess to warrant sacrificing data to tuning... I say "sacrificing"
because if I tune using e.g. 10x repeated 10-fold cross validation, then each
forest only sees what, 90% of the data and each tree also sees 10% fewer data.
Ah! But with `tuneRanger`, I can tune on oob results and skip the cv! Still, the
question remains, can I perhaps improve my out of sample error rates/ AUC/
Cohen's kappa more by simply including more of the data in each tree/forest?

1. I've heard that over/undersampling to address class imbalance can help,
although I'm skeptical this is the case based on SO posts such as [this
one](https://stats.stackexchange.com/q/357466/171222). If I were to just try all
three (no data resampling, over, and under), is it better to compare out of
sample predictive ability by
     a. holding out some portion of my data from all the training steps, and
     then using this held out dataset as a one-off unbiased test (subject to
     high variance, though) of the final model from each sampling procedure
     (three tests total)
     a. Assessing the out of sample predictions through leave-one-out
     cross-validation, where I guess I fix the hyperameters and retrain on each
     subset of the data, testing more the combination of sampling
     method/hyperparameters than a specific RF classifier?
     a. Assess out of sample predictions through k-fold cross validation, with
     the scope of inference as (b), above
         - If this is done wth repeated k-fold cross validation, should I worry
         how the repeated sampling of some datapoints in either side of the
         split is going to bias my estimate?
         
# Test with simulated data

First step is to set some rules for how the learners are supposed to learn. 
         
```{r set set up the training setups}
# function to fit RF models 

fit_rf <- function(data, formu, sampling = NULL, tuneMethod = "none", repeats = NULL){ # set up to take formula as a string

  # Define tuning parameter grid
  # mtry = Number of variables randomly sampled as candidate variables per tree
  fitGrid <- expand.grid(mtry=seq(2, 25, 1))
  
  # Define tuning and training method

  
  fC = caret::trainControl(method = tuneMethod
      , number = ifelse(tuneMethod =="repeatedcv", 10, NA)
      , repeats = ifelse(is.null(repeats), ifelse(tuneMethod != "none", 10, NA), repeats)
      , sampling =  sampling
      , classProbs = TRUE
      , summaryFunction = twoClassSummary)
  
  
  # Train RF model
  rf_fit <- caret::train(formu 
                         , data = data
                         , method = "rf" # this implements randomForest::randomForest but with controls 
                         , distribution = "bernoulli" 
                         , metric = "ROC" # for more robust results with class imbalance
                         , tuneGrid =if(tuneMethod !="none"){fitGrid} else{NULL} # try trees with different numbers of variables
                         , trControl = fC # cross validation and samplling
                         , na.action = na.pass # shouldn't be an issues here
                         # , verbose = T 
                         , importance = T
                         # might tell me something interesting.
                         )
  
  # Plot RF tuning results
  rf_fit$bestTune
  rf_tune <- rf_fit$results
 
  
  
  # Return fit
  return(rf_fit)
  
}
```

Next, I want to write model formulas that match this dataset, and fit them as
above. Begin exploring how they did or at least think they did on the training
data.

```{r set up models}

# no feature selection- use all variables, except employee count 
# since it is always 1 in train data
# standardHours always same
# over18 always yes
my_mod<-as.formula(paste0("as.factor(Attrition) ~ "
                  , paste(names(example_train_dat)[c(1, 3:8, 10:21
       , 23:26, 28:length(names(example_train_dat)))]
                          , collapse = "+")))

# hard code model list
mlist<-c("repeatedcv_up", "none_up", "oob_up"
         ,"repeatedcv_down", "none_down", "oob_down"
         , "repeatedcv_base", "none_base", "oob_base")



all_mods<-map(mlist
               , function(mod){

  mopt = str_split(mod, "_")
  cl <- makePSOCKcluster(7)
  registerDoParallel(cl)
  rf = fit_rf(formu = my_mod
          , data = example_train_dat
          , if(mopt[[1]][2]=="base"){sampling = NULL}else{
            sampling = ifelse(mopt[[1]][2]=="up", "up", "down")}
          , tuneMethod = mopt[[1]][1]
  )
  return(rf)
  stopCluster(cl)
})


# for the repeaated cv method, I can see how much the hyperparameter matters

map(all_mods, function(rf){
  if(!is.null(rf$results$ROC)){
    rf$results %>% ggplot(aes(x=mtry, y=ROC)) +
    labs(x="Number of variables\nsampled at each split" , y="auc"
         , main="Random forest model tune") +
    geom_line() +
    geom_point() +
    theme_bw()}
  })

```

My first impression from those graphs is that, yeah, tunning matters! The
default of $\sqrt(N_{features})$ seems like it's not that far from optimal here,
but that seems lucky. I don't know if there is anything we can generalize about
the number of features, the number of cases, and the extent to which the default
can be off from the optimal values. One powerful-seeming idea is that if there
are a lot of weakly- and strongly-informative features, a small `mtry` lets you
exploit the weak ones, whereas if there are only a few features with any real
predictive value, it's much better to use a large `mtry` than to build a model
by averaging a lot of bad trees. Since we probably wouldn't use RF if we already
knew which world we're in, it seems hard to game.

# start the competitions

Now we'll use the test data to compete the models, and also look at the summary
stats.

```{r other things about these fits}
map(1:length(mlist), function(x){
  print(mlist[x])
  print(all_mods[[x]]$finalModel)
  print(all_mods[[x]]$results)
  })

# based on oob and confusion, looks like upsampling might help

# let's just do the predictions

preds<-map_dfr(1:length(mlist), function(x){
  pre = predict(all_mods[[x]]
                , test_dat[,c(1, 3:8, 10:21
                         , 23:26, 28:length(names(example_train_dat)))]
                , type = "prob")
  predictions = prediction(pre[,2], test_dat[,2])
  preval = predict(all_mods[[x]]
                , test_dat[,c(1, 3:8, 10:21
                         , 23:26, 28:length(names(example_train_dat)))]
                )
  auc = performance(predictions, measure = "auc")@y.values[[1]] 
  data.frame(pre
             , truth = test_dat[,2]
             , accuracy = preval ==test_dat[,2]
             , predictions = predictions@predictions 
             , auc
             , mod = mlist[[x]])
})
  
preds %>% 
  group_by(mod, truth) %>% 
  summarize(rate = round(mean(accuracy),2)
            , instances = n()
            , auc = mean(auc))
```


# impressions

It looks like I can't really do anything that severely improves or degrades 
model performance here. This seems to fit with my sense that different 
cross-validation methods generally point in the same direction. It's not at all 
obvious to me that in the world of small datasets, I should tend to prefer one
method of cross-validation over another. Probably, if it does make a difference,
this suggests the model may not be good anyways. 

I think it's nice to have a held-out dataset as this held-out data can be used
to test any model regardless of how the training data are used in the training
and validation stages, but I can see how it is a bit scary to do this when there
are few data. The workflow of tunign via CV and then building a final model with
all trainign data seems pretty efficient in terms of using data, too.

It's pretty hard to tell from this whether up/downsampling helps, but it's not 
obvious it does, and theory suggests it won't, so I think we should avoid it. 

# more pieces to explore 

Another thing for me to figure out now is a workflow where instead of fitting a
model with cross-validation, I just want to test it with cross-validation. I
think for this we can just use LOOCV with `caret`.  

```{r test models without tuning, but with cross validation}
# assess_with_LOOCV <- map(c("none", "down", "none") , function(samp){
#   caret::train(my_mod 
#                 , data = example_train_dat
#                 , method = "rf" # this implements randomForest::randomForest but with controls 
#                 , distribution = "bernoulli" 
#                 , metric = "ROC" # for more robust results with class imbalance
#                 , tuneGrid = expand.grid(.mtry = 5)
#                , tuneLethgnth = 1
#                 , trControl = caret::trainControl(method = "LOOCV"
#                   , sampling = if(samp == "none"){NULL} else{samp}
#                   , classProbs = TRUE
#                   , summaryFunction = twoClassSummary)
#               , na.action = na.pass # shouldn't be an issues here
#               # , verbose = T 
#               , importance = T)
# })

map(assess_with_LOOCV, function(x){
  print(x$results)})

preds_2<-map_dfr(1:length(assess_with_LOOCV), function(x){
  pre = predict(assess_with_LOOCV[[x]]
                , test_dat[,c(1, 3:8, 10:21
                         , 23:26, 28:length(names(example_train_dat)))]
                , type = "prob")
  predictions = prediction(pre[,2], test_dat[,2])
  preval = predict(assess_with_LOOCV[[x]]
                , test_dat[,c(1, 3:8, 10:21
                         , 23:26, 28:length(names(example_train_dat)))]
                )
  auc = performance(predictions, measure = "auc")@y.values[[1]] 
  data.frame(pre
             , truth = test_dat[,2]
             , accuracy = preval ==test_dat[,2]
             , predictions = predictions@predictions 
             , auc
             , mod = x)
})

preds_2%>% 
  group_by(mod, truth) %>% 
  summarize(rate = round(mean(accuracy),2)
            , instances = n()
            , auc = mean(auc))
```

# summary so far

The thing that seems nice about doing 10-fold cv rather than doing the LOOCV is
getting an estimate of the variability in AUC. It's really hard for me to know
when a difference between two scores is noise vs. is the real deal, and there's
no opportunity for that with only a single replicate AUC. I think I'd be
comfortable with throwing out a tiny bit of data to get this estimate in any
workflow, because point estimates without any sense of variablity make me
nervous. Actually, this is a good argument for skipping the holdout on a small
dataset if we feel ok about just using default hyperparameters, is that rather
than getting a single estimate of model perofrmance, we can get a mean with a
distribution about it.

# nested cross-validation

If we're really trying to get a hold on whether we can make decent predictions
from our data with a given approach, and we know we're very tight on data, we 
probably want to evaluate model performance via cross-validation, in addition to
tuning via cross-validation. I think for most workflows, keeping a held-out 
dataset while doing all the troubleshooting etc. makes the most sense, as this
ensures you don't cheat. But assuming we've commited to an algorithm ahead
of time, and we documented this, and we have hyper-parameters to tune, I think
doing nested cross-validation should be fine. 

For reasons I don't yet understand, it seems like **caret** has not been the
package of choice for doing this, even by Max Kuhn (author). Instead, it seems
people have invested in setting this up in the newer **tidymodels** package.
Based on this [awesome exchange on
github](https://github.com/sebastiansauer/reanalysis-mindful-machine-learning/issues/1#issue-506641549)
I'm tempted to try doing it in **caret** first. For **tidymodels**, I'll follow
[this vignette](https://www.tidymodels.org/learn/work/nested-resampling/) if I
go down that path.


```{r try nested cv with caret}

# 10x repeated 10-fold cross validation on outer (100 datasets)
outer_folds <- createMultiFolds(example_train_dat$Attrition, k = 5, times = 1)


# train model on each fold, get performance estimates

mod <- "repeatedcv_base"

# do all the fits
tic()

fold_fits <- map( outer_folds, function(fold){
    mopt = str_split(mod, "_")
    cl <- makePSOCKcluster(7)
    registerDoParallel(cl)
  
    rf = fit_rf(formu = my_mod
            , data = example_train_dat
            , if(mopt[[1]][2]=="base"){sampling = NULL}else{
              sampling = ifelse(mopt[[1]][2]=="up", "up", "down")}
            , tuneMethod = mopt[[1]][1]
            , repeats =1
    )
    stopCluster(cl)
    return(rf)
  
})

toc()
# create a list of predictions

plan(strategy = "multiprocess", workers = 7)

auc_fun<- possibly(.f=function(x){performance(x, measure = "auc")@y.values[[1]]}, otherwise = NA)

pred_list <- map2_dfr(.x = 1:length(fold_fits)
  , .y = outer_folds
  , .f=function(x, y){ # compute predictions on each of the 100 test sets
   pre = predict(fold_fits[[x]]
                , example_train_dat[-y, c(1, 3:8, 10:21
                         , 23:26, 28:length(names(example_train_dat)))]
                , type = "prob")
  predictions = prediction(pre[,2], factor(example_train_dat[-y, 2], levels = c("yes", "no")))
  preval = predict(fold_fits[[x]]
                , example_train_dat[-y, c(1, 3:8, 10:21
                         , 23:26, 28:length(names(example_train_dat)))]
                )
  my_auc =  roc(response = example_train_dat[-y, 2], predictor = pre$Yes)
  data.frame(#pre
             #, truth = 
             #, 
              accuracy = sum(preval == example_train_dat[-y, 2])/length(y)
             , oob= mean(fold_fits[[x]]$finalModel$err.rate[-y, 1])
             , positive = mean(fold_fits[[x]]$finalModel$err.rate[-y, 2])
             , negative = mean(fold_fits[[x]]$finalModel$err.rate[-y, 3])
             #, predictions = predictions@predictions 
             , auc= as.numeric(my_auc$auc)
             , mod = x
             , mtry = fold_fits[[x]]$finalModel$mtry
             )

  })

# compare to actual predictions
preds_2<-map_dfr(1:length(fold_fits), function(x){
  pre = predict(fold_fits[[x]]
                , test_dat[,c(1, 3:8, 10:21
                         , 23:26, 28:length(names(example_train_dat)))]
                , type = "prob")
  predictions = prediction(pre[,2], test_dat[,2])
  preval = predict(fold_fits[[x]]
                , test_dat[,c(1, 3:8, 10:21
                         , 23:26, 28:length(names(example_train_dat)))]
                )
  auc = performance(predictions, measure = "auc")@y.values[[1]] 
  data.frame(pre
             , truth = test_dat[,2]
             , accuracy = preval ==test_dat[,2]
             , predictions = predictions@predictions 
             , auc
             , mod = x)
})

preds_2%>% 
  group_by(mod, truth) %>% 
  summarize(rate = round(mean(accuracy),2)
            , instances = n()
            , auc = mean(auc))

```
