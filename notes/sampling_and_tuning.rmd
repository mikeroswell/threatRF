---
title: "testing_and_validation"
author: "Michael Roswell"
date: "2/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE
                      , dev = "ragg_png")
knitr::opts_knit$set(root.dir = '../') 
library(tidyverse)
library(caret)
library(doParallel)
library(pROC)
library(ROCR)

```

I downloaded this dataset
https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset/download
to have an unbalanced, simulated dataset to work with. First, just going to look
at it really quickly.

```{r load and peek, results = FALSE}

# load data
dat<-read.csv("data/WA_Fn-UseC_-HR-Employee-Attrition.csv")
head(dat)
str(dat)
# this is the target
table(dat$Attrition)

# there are more cases here, but also the clss imbalance is more severe. 
# before continuing, let's just drop a ton of data from the "No" class. 

no_row<-which(dat$Attrition=="No")
yes_row<-which(dat$Attrition == "Yes")
nkeep<-sample(no_row, 200)
ykeep<-sample(yes_row, 100)
example_train_dat<-bind_rows(
  # the nos
  dat[ykeep,]
  # te yeses
  , dat[nkeep, ]
)

test_dat<-dat[-c(ykeep, nkeep),]
```

Overall, I want to use Random Forest to create a predictive model for
classification. In setting this up, there are a few decisions to make, and
because there are so few data, I'm not sure I can just try things and test them
to evaluate which decision is best in a reasonable manner.

Three basic questions are:

1. How important is model tuning in this context? Based on this
[article](https://bmcmedinformdecismak.biomedcentral.com/track/pdf/10.1186/s12911-021-01688-3.pdf),
I'm wondering if with datasets this small, will the gain in predictive power
from tuning be too low/ hard to assess to warrant sacrificing data to tuning...
I say "sacrificing" because if I tune using e.g. 10x repeated 10-fold cross
validation, then each forest only sees what, 90% of the data and each tree also
sees 10% fewer data. Can I perhaps improve my out of sample error rates/ AUC/
Cohen's kappa more by simply including more of the data in each tree/forest?

1. I've heard that over/undersampling to address class imbalance can help,
although I'm skeptical this is the case based on SO posts such as [this
one](https://stats.stackexchange.com/q/357466/171222). If I were to just try all
three (no data resampling, over, and under), is it better to compare out of
sample predictive ability by
     a. holding out some portion of my data from all the training steps, and
     then using this held out dataset as a one-off unbiased test (subject to
     high variance, though) of the final model from each sampling procedure
     (three tests total)
     a. Assessing the out of sample predictions through leave-one-out
     cross-validation, where I guess I fix the hyperameters and retrain on each
     subset of the data, testing more the combination of sampling
     method/hyperparameters than a specific RF classifier?
     a. Assess out of sample predictions through k-fold cross validation, with
     the scope of inference as (b), above
         - If this is done wth repeated k-fold cross validation, should I worry
         how the repeated sampling of some datapoints in either side of the
         split is going to bias my estimate?
         
```{r set set up the training setups}
# function to fit RF models taken from Chris Free 
# https://github.com/cfree14/domoic_acid/blob/29e49da9ec4b16b6d416d389d37867e4b4b7f95d/code/functions/fit_rf.R

fit_rf <- function(data, formu, sampling = NULL, tune =T){ # set up to take formula as a string

  # Define tuning parameter grid
  # mtry = Number of variables randomly sampled as candidate variables per tree
  fitGrid <- expand.grid(mtry=seq(2, 25, 1))
  
  # Define tuning and training method

  
  fC = caret::trainControl(method = ifelse(tune, "repeatedcv", "none")
      , number = ifelse(tune, 10, NA)
      , repeats = ifelse(tune, 10, NA)
      , sampling =  sampling
      , summaryFunction = twoClassSummary)
  
  
  # Train RF model
  rf_fit <- caret::train(formu 
                         , data = data
                         , method = "rf" # this implements randomForest::randomForest but with controls 
                         , distribution = "bernoulli" 
                         #, metric = "kappa" # for more robust results with class imbalance
                         , tuneGrid =if(tune){fitGrid} else{NULL} # try trees with different numbers of variables
                         , trControl = fC # cross validation and samplling
                         , na.action = na.pass # shouldn't be an issues here
                         # , verbose = T 
                         , importance = T
                         # might tell me something interesting.
                         )
  
  # Plot RF tuning results
  rf_fit$bestTune
  rf_tune <- rf_fit$results
 
  
  
  # Return fit
  return(rf_fit)
  
}
```

```{r set up models}

# no feature selection- use all variables, except employee count 
# since it is always 1 in train data
# standardHours always same
# over18 always yes
my_mod<-as.formula(paste0("as.factor(Attrition) ~ "
                  , paste(names(example_train_dat)[c(1, 3:8, 10:21,
                                                     23:26, 28:length(names(example_train_dat)))]
                          , collapse = "+")))

# hard code model list
mlist<-c("tune_up", "one_up", "tune_down", "one_down", "tune_base", "one_base")



all_mods<-map(mlist
               , function(mod){

  mopt = str_split(mod, "_")
  cl <- makePSOCKcluster(7)
  registerDoParallel(cl)
  rf = fit_rf(formu = my_mod
          , data = example_train_dat
          , if(mopt[[1]][2]=="base"){sampling = NULL}else{
            sampling = ifelse(mopt[[1]][2]=="up", "up", "down")}
          , tune =if_else(mopt[[1]][1]=="tune", TRUE, FALSE)
  )
  return(rf)
  stopCluster(cl)
})


all_mods[[1]]$results %>% 
  ggplot(aes(x=mtry, y=auc)) +
    labs(x="Number of variables\nsampled at each split" , y="auc", main="Random forest model tune") +
    geom_line() +
    geom_point() +
    theme_bw()

map(all_mods, function(rf){
    rf$results %>% ggplot(aes(x=mtry, y=AUC)) +
    labs(x="Number of variables\nsampled at each split" , y="AUC", main="Random forest model tune") +
    geom_line() +
    geom_point() +
    theme_bw()})

```

My first impression from those graphs is that, yeah, tunning matters! It looks
like huge variation in kapp with mtry, and it looks kind of directional too. The
default of $\sqrt(N_{features})$ seems like it's not that far from optimal here,
but that seems lucky.

```{r other things about these fits}
map(all_mods, function(x){
  print(x$finalModel)})

# based on oob and confusion, looks like upsampling might help

# let's just do the predictions

preds<-map_dfr(1:6, function(x){
  pre = predict(all_mods[[x]]
                , test_dat[,c(1, 3:8, 10:21
                         , 23:26, 28:length(names(example_train_dat)))]
                , type = "prob")
  predictions = prediction(pre, test_dat[,2])
  auc = performance(predictions, measure = "auc")
  data.frame(pre
             , truth = test_dat[,2]
             , accuracy = pre ==test_dat[,2]
             , predictions, auc)
})
  
preds %>% 
  group_by(mod, truth) %>% 
  summarize(rate = round(mean(accuracy),2)
            , instances = n()
            , auc = mean(auc))
```

This seems to support the idea that tuning isn't worth the the computing time or
the data reduction, at least for this dataset. But perhaps it's just that the
defaults were good for this dataset. And, it's pretty hard to tell from this
whether up/downsampling helps, but it's not obvious it does.
